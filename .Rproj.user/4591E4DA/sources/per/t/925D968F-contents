---
title: "Clinical State: Part Two - Embedding"
author: "Dan W Joyce"
date: 2019-05-12T18:00:00+01:00


header-includes: \usepackage{amsmath}
image:
  caption: ''
  focal_point: ''
categories: [clinical state, trajectories, dynamical systems]
# slug: trajectories-part-one
# substitle: Dynamical Systems
tags: [state, modelling]
bibliography: [./traj.bib]
---



```{r setup, include=FALSE}
rm( list = ls() )

reqd.packages <- c("ggplot2", 
                   "ggrepel",
                   "reshape2",
                   "latex2exp", 
                   "deSolve", 
                   "kableExtra",
                   "minpack.lm",
                   "gridExtra",
                   "pracma",
                   "viridis",
                   "dse",
                   "depmixS4",
                   "blogdown")


# to install required packages
todo.packages <- reqd.packages[!(reqd.packages %in% installed.packages()[,"Package"])]
if(length(todo.packages) > 0) install.packages(todo.packages)

lapply(reqd.packages, require, character.only = TRUE)

knitr::opts_chunk$set(echo = TRUE)


# globals for presentation
dwjtheme <- theme_minimal() + 
  theme(axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        plot.title = element_text(size = rel(1.25), face = "bold", hjust = 0.5 ))

# get some data
d <- as.data.frame( read.csv( "../../../Trajectory/ex-data.csv") )
```

\newcommand{\M}[1]{\textbf{#1}}

In the last post, we showed examples of how concepts from dynamical systems can be operationalised.  However, we need to have a clear model of the dynamics underlying the state and phase space to make these useful.  My view is that we rarely have this for complex clinical state - except metaphorically (i.e. mood regulation behaves *as if* it were a damped oscillator).  If we can't pin down a realistic ODE for a single state variable, it seems implausible we could find systems of coupled ODEs that would act as a useable model for clinical state and trajectory.


## Clinical State *vs* Latent State

Clinical state has so far been defined as a multidimensional observation of a person via some instrument and in the last blog, we identified clinical state with observations.  We weren't explicit about the process by which measurements are generated and previously assumed the dynamics (underlying the evolution of the trajectory in the clinical state space) was in a "one to one" mapping and further, that the dynamics had a strong model (recall, we used second-order ordinary differential equations).   

However, it might be better to conceive of the observed multidimensional observation as a measurement on some underlying latent system of states; by analogy, most will be familiar with the idea of [factor analysis](https://en.wikipedia.org/wiki/Factor_analysis), where a collection of correlated variables are explained by a smaller number of 'underlying' variables (or factors) -- for example in the study of the ["16PF" and "big-five" factors](https://en.wikipedia.org/wiki/16PF_Questionnaire) in personality research.

Similarly, one can construct a linear dynamical system (using the state space idea) with the term "state" and "state space" being reserved for some unobserved (hidden, latent) state that underlies the evolution of the observations in time. 

```{r echo = FALSE, out.width = "60%", fig.align = "center"}
knitr::include_graphics("../../../Trajectory/Kalman-LDS.png")
```

In this diagram, notice how there is a 'loop' of time-evolving state process $\M{x}_t$ and supervening on this is a separate process generating observable measurements $\M{y}_t$.  


## Basic Setup

  * Assume that there are measurements $\M{y}_t$ of dimension $p$ -- so they exist in a $p$-dimensional space as discussed before
  * Underneath these measurements is a lower $k$-dimensional, unobserved, latent **state** $\M{x}_t$ that evolves in time

The state obeys the Markov assumptions, so the state at time $t$ ($\M{x}_t$) depends only on it's preceding state $\M{x}_{t-1}$. At any moment in time, we can describe a linear dynamical system where:

  * The state evolves over time as:
    $$\M{x}_{t+1} = \M{A}\M{x}_t + \M{w}$$
    where $\M{A}$ is a $k \times k$ state transition matrix
    
  * The observed output of the system is dependent on the state $\M{x}_t$ as: 
    $$\M{y}_t = \M{C} \M{x}_t + \M{v}$$
    where $\M{C}$ is a $p \times k$ observation (or measurement) matrix



The length $k$ vector $\M{w}$ is the state evolution noise, and similarly, $\M{v}$ is the observation noise.  Both are assumed independent of each other and $\M{x}$ and $\M{y}$.  

It is worth formally stating the noise vectors distribution:
$$
\begin{aligned}
  \M{w} \thicksim & \mathcal{N}( \M{0}, \M{Q} ) \\
  \M{v} \thicksim & \mathcal{N}( \M{0}, \M{R} )
\end{aligned}
$$




The model described above represents a Kalman filter [@haykin2001kalman], a linear Gaussian process (if assumptions are made on the probabilistic interpretation of the state evolution and observation equations), and all belong to the **general linear Gaussian state space model** approach to time series [@durbin2012time] -- see Chapter 3, equation 3.1. Sidenote : it can be shown that a number of dimensionality-reduction techniques (including principle components and factor analysis) as well as the Kalman filter (CHECK THIS) can be cast in the framework of linear Gaussian models of the kind described above -- see [@roweis1999unifying] -- when assumptions are made about $\M{A}$, $\M{C}$, the noise parameters $\M{Q}$ and $\M{R}$ and the estimation of various parameters is cast in the expectation-maximisation framework.  

Intuitively, when we have a sequence of length $\tau$ of multivariate (vector) observed values $\M{y}_1 , \M{y}_2 \ldots, \M{y}_\tau$ we can find a reduced hidden space underlying these observations.  

The *objective* for modelling clinical state is central to the approach taken, and there are two primary objectives:

  * Prediction and forecasting -- where the measure of a 'good algorithm' is extensional: the quality of predictions or forecasts marrying with observations is more important than the machinery (i.e. algorithm or model), sometimes called "black box"
  * Inference, usually, in the tradition of statistical methods for model inference is intensional -- where the hypothesised model is itself valuable for making inferences but usually in the context of how well the model describes the data.
  
This similarly mirrors a debate in the current AI/ML *vs.* statistical literatures.  

Related to these objectives, there are two important considerations in the linear Gaussian model apporach [@roweis1999unifying]:

  * inference : filtering and smoothing -- when the parameter matrices $\M{A}$, $\M{C}$, $\M{Q}$ and $\M{R}$ are known, then we can make inferences about the unknown hidden states
  * learning or system identification -- when the parameter matrices are *unknown* and we have only the sequence of observations $\M{y}_1 , \M{y}_2 \ldots, \M{y}_\tau$

Advantage of the state-space approach : "The key advantage of the state space approach is that it is based on a structural analysis of the problem. The different components that make up the series, such as trend, seasonal, cycle and calendar variations, together with the effects of explanatory variables and interventions, are modelled separately before being put together in the state space model." In contrast : Box-Jenkins [ARMA, ARIMA] approach : "In contrast, the Box–Jenkins approach is a kind of ‘black box’, in which the model adopted depends purely on the data without prior analysis of the structure of the system that generated the data." Durbin and Koopman (2012)

For our purposes, an important sub-model is the hidden markov model, where the states $\M{x}$ are discrete, and the usual estimation procedures are specialisations of the Expectation Maximisation process applied to an equivalent Gaussian state space model -- see appendix of [@roweis1999unifying]. 


## Some demos of HMMs
```{r}
require( depmixS4 )
data("speed")
set.seed(1)
mod <- depmix(response = rt ~ 1, data = speed, nstates = 2, trstart = runif(4))
fm <- fit(mod, emc=em.control(rand=FALSE))
```

### Multivariate
```{r}
mod <- depmix(list(rt ~ 1,corr ~ 1), data = speed, nstates = 2,
              family = list(gaussian(), multinomial("identity")),
              transition = ~ scale(Pacc), 
              instart = runif(2))
fm <- fit(mod, verbose = FALSE, emc=em.control(rand=FALSE))
```

## Some Kalman ARMA type experiments

Some implementation details for R and Kalman : https://magesblog.com/post/2015-01-06-kalman-filter-example-visualised-with-r/

```{r}
require(dse)

fileName <- system.file("otherdata", "eg1.dat", package="dse")
eg1.DSE.data <- t(matrix(scan(fileName),5, 364))[, 2:5]

# convert to dse based TS object
eg1.DSE.data <- TSdata(input= eg1.DSE.data[,1,drop = F],
                       output= eg1.DSE.data[, 2:4, drop = F])

# tidy time labels
eg1.DSE.data <-tframed(eg1.DSE.data,
                      list(start=c(1961,3), frequency=12))

seriesNamesInput(eg1.DSE.data) <- "R90"
seriesNamesOutput(eg1.DSE.data) <- c("M1","GDPl2", "CPI")

# run model estimation
model1 <- estVARXls(eg1.DSE.data)
model2 <- estSSMittnik(eg1.DSE.data, n=4)

```

```{r echo = FALSE}
LocalLinearGradient <- function( F ) {
  # compute local linear numerical approximation to gradient of F with window +/- 1
  N <- length(F)
  dF <- rep(NA, N)  
  for( i in 2:(N-1) ) {
    dF[i] <- ( F[i+1] - F[i-1] ) / 2
  }
  # boundary cases
  dF[1] <- (F[2] - F[1])
  dF[N] <- (F[N] - F[N-1])
  return( dF )
}
```

$$
\begin{aligned}
  \dot{y_2} &= \zeta y_2 + \eta y_1 \\
  \dot{y_1} &= y_2
\end{aligned}
$$

The analytic solution for this system is well understood and depends on the parameters $\zeta$ and $\eta$ -- there are three solutions for $x(t)$ depending on whether the system is damped, under-damped or critically damped -- [@Lebl2019diff] gives a helpful tutorial.  However, as we won't know the parameters in advance, we need to use numerical methods (an ODE solver) reassured that we can plug in any set of parameters to construct and visualise $x(t)$.

# Simulated Example


```{r echo = FALSE}
# Direct implementation
OscDiffEq <- function (t, y, params) {
  zeta <- params["zeta"]
  eta  <- params["eta"]
  list(
          c( y[2],
             zeta * y[2] + eta * y[1]
          )
  )
}
```



```{r echo = FALSE, fig.align='center', out.width="70%"}
actual.zeta <- -0.1
actual.eta  <- -0.5

params <- c( zeta = -0.1, eta = -0.5 )
times <- seq(1,100, by = 0.5)
soln.1 <- ode(y = c(y1 = 5, y2 = -2.5), func = OscDiffEq, times = times, parms = params)

# keep y1 and y2 (the second and first derivative respectively)
ex.osc <- data.frame(
            X.1  = soln.1[,"y1"], # + rnorm( n, 0, 0.25),
            dX.1 = soln.1[,"y2"],
            Time = soln.1[,"time"]
)

p.x <- ggplot( ex.osc ) +
        geom_line( aes( x = Time, y = X.1 ), colour = "#984ea3" ) +
        annotate("point", x = 1, y = 5, size = 4, colour = "#984ea3") +
        ylab(TeX("x(t)")) +
        xlab(TeX("t")) + 
        ggtitle("Simulated Data: Damping") +
        dwjtheme

p.phase <- ggplot( ex.osc ) +
        geom_path( aes( x = X.1, y = dX.1 ), colour = "#984ea3" ) +
        annotate("point", x = ex.osc$X.1[1], y = ex.osc$dX.1[1], size = 4, colour = "#984ea3") +
        ylab("dx(t)") +
        xlab("x(t)") + 
        ggtitle("Phase Plane: Damping") +
        dwjtheme

grid.arrange( p.x, p.phase, ncol = 2)

```

We generate some simulated data with the following parameters:

  * $\zeta$ = `r actual.zeta` (the 'damping' or 'amplification')
  * $\eta$ = `r actual.eta` (the 'frequency' of oscillations)

## Embedding

```{r echo = FALSE, fig.align='center', out.width="70%"}

tau <- 5

xt    <- ex.osc$X.1
xt.T  <- xt[ tau:length(xt) ]
xt.2T <- xt[ (2*tau):length(xt) ]

delay.df <- data.frame(
  xt     = xt[1:length(xt.2T)],
  xt.T   = xt.T[1:length(xt.2T)],
  xt.2T  = xt.2T[1:length(xt.2T)]
)

ggplot( delay.df, aes( x = xt.T, y = xt.2T ) ) +
  geom_path() +
  dwjtheme


```

# References

